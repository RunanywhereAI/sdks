# Voice Pipeline Architecture - RunAnywhere Swift SDK

## Overview

The Voice Pipeline in the RunAnywhere Swift SDK provides comprehensive voice processing capabilities including Voice Activity Detection (VAD), Speech-to-Text (STT), Large Language Model processing (LLM), Text-to-Speech (TTS), and Speaker Diarization. The architecture follows the SDK's standard 5-layer pattern for maximum modularity, testability, and maintainability.

## Table of Contents
1. [Architecture Layers](#ğŸ—ï¸-architecture-layers)
2. [Voice Processing Pipeline](#ğŸ”„-voice-processing-pipeline)
3. [Configuration System](#ğŸ›ï¸-configuration-system)
4. [Event System](#ğŸ“Š-event-system)
5. [Service Integration](#ğŸ”Œ-service-integration)
6. [Testing Strategy](#ğŸ§ª-testing-strategy)
7. [Analytics & Monitoring](#ğŸ“ˆ-analytics--monitoring)
8. [Usage Examples](#ğŸš€-usage-examples)
9. [Platform Support](#ğŸ”§-platform-support)
10. [Migration Guide](#ğŸ-migration-guide)

## ğŸ—ï¸ Architecture Layers

### 1. Foundation Layer
**Location**: `Sources/RunAnywhere/Foundation/`

**Components**:
- **ServiceContainer.swift**: Dependency injection container with voice capability integration
- **AdapterRegistry.swift**: Registration and discovery of voice adapters

**Responsibilities**:
- Dependency injection for voice components
- Service discovery and lifecycle management
- Cross-cutting concerns like logging and configuration

### 2. Infrastructure Layer
**Location**: `Sources/RunAnywhere/Infrastructure/Voice/`

**Platform Audio Management**:
```
Infrastructure/Voice/Platform/
â”œâ”€â”€ iOSAudioSession.swift          # iOS-specific audio session management
â””â”€â”€ macOSAudioSession.swift        # macOS-specific audio session management
```

**Service Adapters**:
```
Infrastructure/Voice/Adapters/
â””â”€â”€ SystemTTSAdapter.swift         # System Text-to-Speech implementation
```

**Responsibilities**:
- Platform-specific audio session configuration
- Audio permissions and interruption handling
- System service integrations (TTS, audio I/O)
- Hardware abstraction layer

### 3. Core Layer
**Location**: `Sources/RunAnywhere/Core/`

**Service Protocols**:
```
Core/Protocols/Voice/
â”œâ”€â”€ VoiceService.swift              # Main voice service interface + VoiceError enum
â”œâ”€â”€ TextToSpeechService.swift       # TTS service contract
â”œâ”€â”€ VADService.swift               # Voice Activity Detection contract
â”œâ”€â”€ SpeakerDiarizationProtocol.swift # Speaker identification contract
â””â”€â”€ WakeWordDetector.swift         # Wake word detection interface
```

**Responsibilities**:
- Protocol definitions for voice services
- Core error handling (`VoiceError` enum)
- Service contracts and interfaces
- Framework-agnostic abstractions

### 4. Capabilities Layer
**Location**: `Sources/RunAnywhere/Capabilities/Voice/`

**Main Services**:
```
Capabilities/Voice/Services/
â”œâ”€â”€ VoiceCapabilityService.swift    # Main voice capability orchestrator
â”œâ”€â”€ VoicePipelineManager.swift      # Modular pipeline manager (was ModularVoicePipeline)
â”œâ”€â”€ VoiceAnalyticsService.swift     # Voice processing analytics
â”œâ”€â”€ VoiceSessionManager.swift       # Session lifecycle management
â””â”€â”€ DefaultSpeakerDiarization.swift # Default speaker diarization implementation
```

**Processing Handlers**:
```
Capabilities/Voice/Handlers/
â”œâ”€â”€ VADHandler.swift               # Voice Activity Detection processing
â”œâ”€â”€ STTHandler.swift               # Speech-to-Text processing
â”œâ”€â”€ LLMHandler.swift               # LLM processing and streaming
â”œâ”€â”€ TTSHandler.swift               # Text-to-Speech processing
â””â”€â”€ SpeakerDiarizationHandler.swift # Speaker identification processing
```

**Specialized Operations**:
```
Capabilities/Voice/Operations/
â””â”€â”€ StreamingTTSOperation.swift    # Streaming TTS for real-time speech
```

**Processing Strategies**:
```
Capabilities/Voice/Strategies/
â”œâ”€â”€ VAD/
â”‚   â””â”€â”€ SimpleEnergyVAD.swift      # Energy-based voice activity detection
â””â”€â”€ AudioSegmentation/
    â””â”€â”€ AudioSegmentationStrategy.swift # Audio segmentation strategies
```

**Factories**:
```
Capabilities/Voice/Factories/
â””â”€â”€ DiarizationFactory.swift      # Speaker diarization factory
```

**Data Models**:
```
Capabilities/Voice/Models/
â””â”€â”€ VoiceSession.swift             # Voice session data model
```

**Responsibilities**:
- Voice capability orchestration and coordination
- Pipeline flow management and component delegation
- Processing handlers for each pipeline stage
- Analytics and performance monitoring
- Session management and state tracking

### 5. Public Layer
**Location**: `Sources/RunAnywhere/Public/`

**Public API**:
```
Public/Extensions/
â””â”€â”€ RunAnywhereSDK+Voice.swift     # Public voice API surface
```

**Configuration Models**:
```
Public/Models/Voice/
â”œâ”€â”€ ModularPipelineConfig.swift    # âœ… NEW: Modern modular configuration
â”œâ”€â”€ ModularPipelineEvent.swift     # âœ… NEW: Modular pipeline events
â”œâ”€â”€ VoicePipelineEvent.swift       # Pipeline events
â”œâ”€â”€ VoiceProcessingMode.swift      # Audio processing modes
â”œâ”€â”€ VoiceSTTConfig.swift           # Speech-to-Text configuration
â”œâ”€â”€ VoiceLLMConfig.swift           # LLM configuration
â”œâ”€â”€ VoiceTTSConfig.swift           # Text-to-Speech configuration
â””â”€â”€ VADConfig.swift               # Voice Activity Detection configuration
```

**Audio & Transcription Models**:
```
Public/Models/Voice/
â”œâ”€â”€ AudioChunk.swift               # Audio data structures
â”œâ”€â”€ TranscriptionOptions.swift     # Transcription configuration
â”œâ”€â”€ TranscriptionResult.swift      # Transcription results
â””â”€â”€ TranscriptionSegment.swift     # Transcription segments
```

**Responsibilities**:
- User-facing API surface
- Configuration and event models
- Public type definitions
- API documentation and examples

## ğŸ”„ Voice Processing Pipeline

### Pipeline Flow
```
Audio Input â†’ VAD â†’ STT â†’ LLM â†’ TTS â†’ Audio Output
              â†“     â†“     â†“     â†“
            Events Events Events Events
```

### Component Interaction
```
VoiceCapabilityService
â”œâ”€â”€ Creates â†’ VoicePipelineManager
â”œâ”€â”€ Manages â†’ VoiceSessionManager
â””â”€â”€ Tracks â†’ VoiceAnalyticsService

VoicePipelineManager
â”œâ”€â”€ Delegates â†’ VADHandler
â”œâ”€â”€ Delegates â†’ STTHandler
â”œâ”€â”€ Delegates â†’ LLMHandler
â”œâ”€â”€ Delegates â†’ TTSHandler
â””â”€â”€ Delegates â†’ SpeakerDiarizationHandler

Each Handler
â”œâ”€â”€ Uses â†’ Platform Services (via ServiceContainer)
â”œâ”€â”€ Emits â†’ ModularPipelineEvent
â””â”€â”€ Processes â†’ Specific pipeline stage
```

### Data Flow
1. **Audio Input**: Raw audio chunks from microphone/file
2. **VAD Processing**: Voice activity detection and speech segmentation
3. **STT Processing**: Speech-to-text conversion with optional speaker diarization
4. **LLM Processing**: Language model processing for conversational AI
5. **TTS Processing**: Text-to-speech synthesis for responses
6. **Event Emission**: Real-time events for UI updates and monitoring

## ğŸ›ï¸ Configuration System

### Modern Configuration (ModularPipelineConfig)
```swift
let config = ModularPipelineConfig(
    components: [.vad, .stt, .llm, .tts],
    vad: VADConfig(energyThreshold: 0.02),
    stt: VoiceSTTConfig(modelId: "whisper-base", language: "en"),
    llm: VoiceLLMConfig(modelId: "llama-7b"),
    tts: VoiceTTSConfig(voice: "system"),
    streamingEnabled: true
)
```

### Component Selection
Components can be mixed and matched:
- **Transcription Only**: `[.stt]`
- **Transcription with VAD**: `[.vad, .stt]`
- **Conversational (No TTS)**: `[.vad, .stt, .llm]`
- **Full Pipeline**: `[.vad, .stt, .llm, .tts]`

### Convenience Builders
```swift
// Quick configurations
ModularPipelineConfig.transcriptionOnly()
ModularPipelineConfig.transcriptionWithVAD()
ModularPipelineConfig.conversationalNoTTS()
ModularPipelineConfig.fullPipeline()
```

## ğŸ“Š Event System

### Event Types
```swift
public enum ModularPipelineEvent {
    // VAD events
    case vadSpeechStart, vadSpeechEnd
    case vadAudioLevel(Float)

    // STT events
    case sttPartialTranscript(String)
    case sttFinalTranscript(String)
    case sttLanguageDetected(String)

    // STT with Speaker Diarization
    case sttPartialTranscriptWithSpeaker(String, SpeakerInfo)
    case sttFinalTranscriptWithSpeaker(String, SpeakerInfo)
    case sttNewSpeakerDetected(SpeakerInfo)

    // LLM events
    case llmThinking, llmStreamStarted
    case llmPartialResponse(String)
    case llmFinalResponse(String)
    case llmStreamToken(String)

    // TTS events
    case ttsStarted, ttsCompleted
    case ttsAudioChunk(Data)

    // Pipeline events
    case pipelineStarted, pipelineCompleted
    case pipelineError(Error)

    // Component lifecycle
    case componentInitializing(String)
    case componentInitialized(String)
    case componentInitializationFailed(String, Error)
    case allComponentsInitialized
}
```

### Event Handling
```swift
extension MyViewModel: VoicePipelineManagerDelegate {
    func pipeline(_ pipeline: VoicePipelineManager, didReceiveEvent event: ModularPipelineEvent) {
        switch event {
        case .sttFinalTranscript(let text):
            // Handle transcription
        case .llmPartialResponse(let response):
            // Handle LLM streaming
        case .ttsStarted:
            // Handle TTS start
        // ... other events
        }
    }
}
```

## ğŸ”Œ Service Integration

### Service Discovery
The voice capability integrates with the SDK's service discovery system:

```swift
// Automatic service discovery
let voiceService = serviceContainer.voiceCapabilityService.findVoiceService(for: "whisper-base")
let llmService = serviceContainer.voiceCapabilityService.findLLMService(for: "llama-7b")
let ttsService = serviceContainer.textToSpeechService
```

### Dependency Injection
All components receive dependencies through the ServiceContainer:
- Voice services (STT models)
- LLM services (language models)
- TTS services (text-to-speech)
- Audio session management
- Analytics and monitoring

## ğŸ§ª Testing Strategy

### Unit Testing
Each component can be tested independently:
```swift
// Handler testing
let vadHandler = VADHandler()
let mockVADService = MockVADService()
let result = vadHandler.processAudio(chunk, vad: mockVADService)

// Service testing
let voiceCapability = VoiceCapabilityService()
let pipeline = voiceCapability.createPipeline(config: testConfig)
```

### Integration Testing
```swift
// Full pipeline testing
let sdk = RunAnywhereSDK.shared
let pipeline = sdk.createVoicePipeline(config: ModularPipelineConfig.fullPipeline())
pipeline.delegate = testDelegate
// Test audio processing flow
```

### Mocking
Each layer can be mocked independently:
- Mock voice services for STT testing
- Mock LLM services for conversation testing
- Mock audio sessions for platform testing

## ğŸ“ˆ Analytics & Monitoring

### Voice Analytics
```swift
public struct VoiceMetrics {
    public let totalTranscriptions: Int
    public let averageTranscriptionTime: TimeInterval
    public let totalSpeechDuration: TimeInterval
    public let errorRate: Float
    public let pipelineMetrics: [PipelineMetric]
}
```

### Performance Tracking
- Component initialization times
- Processing latencies per stage
- Memory usage monitoring
- Error rate tracking
- User engagement metrics

## ğŸš€ Usage Examples

### Basic Transcription
```swift
import RunAnywhereSDK

let sdk = RunAnywhereSDK.shared
let config = ModularPipelineConfig.transcriptionOnly()
let pipeline = sdk.createVoicePipeline(config: config)
pipeline.delegate = self

// Start transcription
let audioStream = createAudioStream()
for await event in sdk.processVoice(audioStream: audioStream, config: config) {
    switch event {
    case .sttFinalTranscript(let text):
        print("Transcribed: \(text)")
    default:
        break
    }
}
```

### Conversational AI
```swift
let config = ModularPipelineConfig.fullPipeline(
    sttModel: "whisper-base",
    llmModel: "llama-7b",
    ttsVoice: "system"
)

let pipeline = sdk.createVoicePipeline(config: config)
pipeline.delegate = self

// Full conversational flow with TTS responses
pipeline.startContinuousMode()
```

### Speaker Diarization
```swift
let diarization = try DiarizationFactory.createFluidAudioDiarization()
let config = ModularPipelineConfig.transcriptionWithVAD()

let pipeline = sdk.createVoicePipeline(
    config: config,
    speakerDiarization: diarization
)

// Receive speaker-aware transcripts
func pipeline(_ pipeline: VoicePipelineManager, didReceiveEvent event: ModularPipelineEvent) {
    switch event {
    case .sttFinalTranscriptWithSpeaker(let text, let speaker):
        print("Speaker \(speaker.id): \(text)")
    case .sttNewSpeakerDetected(let speaker):
        print("New speaker detected: \(speaker.id)")
    default:
        break
    }
}
```

## ğŸ”§ Platform Support

### iOS Features
- AVAudioSession integration
- Background audio processing
- Interruption handling
- Microphone permissions

### macOS Features
- AVAudioEngine integration
- Input device selection
- Audio level monitoring
- System audio routing

### Cross-Platform
- Unified API surface
- Platform-specific optimizations
- Consistent behavior across platforms

## ğŸ Migration Guide

### From Legacy ModularVoicePipeline
```swift
// OLD (no longer supported)
let oldPipeline = ModularVoicePipeline(config: legacyConfig)

// NEW
let newPipeline = sdk.createVoicePipeline(config: modularConfig)
```

### Configuration Migration
```swift
// OLD VoicePipelineConfig (REMOVED)
// This configuration format is no longer supported

// NEW ModularPipelineConfig
let newConfig = ModularPipelineConfig(
    components: [.vad, .stt, .llm, .tts],
    stt: VoiceSTTConfig(modelId: "whisper-base"),
    llm: VoiceLLMConfig(modelId: "llama-7b"),
    tts: VoiceTTSConfig(voice: "system")
)
```

### Delegate Migration
```swift
// OLD
extension MyClass: ModularVoicePipelineDelegate { ... }

// NEW
extension MyClass: VoicePipelineManagerDelegate {
    func pipeline(_ pipeline: VoicePipelineManager, didReceiveEvent event: ModularPipelineEvent) {
        // Handle events (same event types)
    }
}
```

## ğŸ” Troubleshooting

### Common Issues

**Pipeline Creation Fails**
- Verify model availability through service discovery
- Check configuration component dependencies
- Ensure proper ServiceContainer initialization

**Audio Processing Issues**
- Check microphone permissions
- Verify audio session configuration
- Monitor audio interruption events

**Performance Issues**
- Use VoiceAnalyticsService to identify bottlenecks
- Consider reducing component complexity
- Monitor memory usage during long sessions

### Debug Logging
```swift
// Enable voice logging
SDKLogger.setLevel(.debug, for: "VoicePipelineManager")
SDKLogger.setLevel(.debug, for: "VoiceCapabilityService")
SDKLogger.setLevel(.debug, for: "VADHandler")
```

## ğŸ§¹ Cleanup Required

### Legacy Components (To Be Removed)
The following files contain old logic and should be deleted:

1. **`Core/Protocols/Voice/VoiceOrchestrator.swift`** - âŒ UNUSED
2. **`Core/Services/Voice/DefaultVoiceOrchestrator.swift`** - âŒ UNUSED
3. **`Core/Protocols/Voice/VoiceActivityDetector.swift`** - âŒ UNUSED
4. **`Core/Protocols/Voice/VoicePerformanceMonitor.swift`** - âŒ UNUSED
5. **`Public/Models/Voice/VoicePipelineConfig.swift`** - âŒ OLD CONFIG

### ServiceContainer Cleanup
Remove the unused `voiceOrchestrator` property from ServiceContainer.swift that creates DefaultVoiceOrchestrator but is never used.

## ğŸ“š Related Documentation

- [SDK Architecture Overview](./ARCHITECTURE_V2.md)
- [Service Container Guide](./SERVICE_CONTAINER.md)
- [Model Management](./MODEL_MANAGEMENT.md)
- [Speaker Diarization Integration](./SPEAKER_DIARIZATION.md)

---

*This architecture documentation reflects the completed voice capability refactoring that transformed the monolithic ModularVoicePipeline into a clean, modular, 5-layer architecture following SOLID principles and the SDK's standard patterns.*

# RunAnywhereAI iOS Sample App

A comprehensive iOS example app showcasing advanced on-device AI capabilities with multiple LLM frameworks, benchmarking tools, and performance optimization features.

## Features

### ğŸ¤– Multi-Framework Support
Integrates 10+ LLM frameworks including:
- **llama.cpp** (GGUF models)
- **Core ML** (Apple's native ML framework)
- **MLX** (Apple Silicon optimized)
- **ONNX Runtime** (Cross-platform inference)
- **TensorFlow Lite** (Mobile-optimized ML)
- **ExecuTorch** (PyTorch mobile runtime)
- **MLC LLM** (Machine Learning Compilation)
- **Swift Transformers** (Native Swift implementation)
- **PicoLLM** (Lightweight inference engine)
- **Foundation Models** (System-level APIs)
- **Mock service** for testing and development

### ğŸ’¬ Advanced Chat Interface
- **Enhanced Chat View**: Full-featured chat UI with streaming responses
- **Chat Interface View**: Specialized chat components
- **Message Management**: Support for enhanced chat messages with metadata
- **Conversation Export**: Export chat histories
- **Conversation Store**: Persistent chat storage

### ğŸ“Š Performance & Benchmarking
- **Comprehensive Benchmarking Suite**: Multi-framework performance testing
- **Real-time Performance Monitoring**: Live metrics during inference
- **Memory Profiling**: Track memory usage and optimization
- **Performance Dashboard**: Visual charts and analytics
- **A/B Testing Framework**: Compare different configurations
- **Benchmark Service**: Automated performance testing

### ğŸ”§ Model Management & Conversion
- **Model Repository**: Centralized model storage and management
- **Model Compatibility Checker**: Validate model-framework compatibility
- **Model Conversion Wizard**: Convert between different model formats
- **Model Format Detection**: Automatic format identification
- **Model Quantization**: Optimize models for mobile deployment
- **Model Import/Download**: Easy model acquisition and management
- **Bundled Models Service**: Pre-packaged model management

### ğŸš€ Framework Exploration
- **Framework Capability Explorer**: Interactive framework comparison
- **Model Compatibility Matrix**: Visual compatibility overview
- **Framework Configurations**: Optimized settings per framework
- **Dependency Container**: Modular service architecture

### âš™ï¸ Advanced Configuration
- **Settings View**: Comprehensive configuration options
- **Generation Options**: Fine-tune inference parameters
- **Memory Management**: Intelligent memory allocation
- **Service Lifecycle Management**: Proper resource handling
- **Logging System**: Comprehensive debug and performance logging

## Project Structure

```
RunAnywhereAI/
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ ChatMessage.swift
â”‚   â”œâ”€â”€ ChatMessageEnhanced.swift
â”‚   â”œâ”€â”€ GenerationOptions.swift
â”‚   â””â”€â”€ ModelInfo.swift
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ LLMService/
â”‚   â”‚   â”œâ”€â”€ BaseLLMService.swift
â”‚   â”‚   â”œâ”€â”€ CoreMLService.swift
â”‚   â”‚   â”œâ”€â”€ FoundationModelsService.swift
â”‚   â”‚   â”œâ”€â”€ FrameworkInfo.swift
â”‚   â”‚   â”œâ”€â”€ LLMCapabilities.swift
â”‚   â”‚   â”œâ”€â”€ LLMInference.swift
â”‚   â”‚   â”œâ”€â”€ LLMMetrics.swift
â”‚   â”‚   â”œâ”€â”€ LLMModelLoader.swift
â”‚   â”‚   â”œâ”€â”€ LLMProtocol.swift
â”‚   â”‚   â”œâ”€â”€ LlamaCppService.swift
â”‚   â”‚   â”œâ”€â”€ MLXService.swift
â”‚   â”‚   â””â”€â”€ MockLLMService.swift
â”‚   â”œâ”€â”€ Benchmarking/
â”‚   â”‚   â””â”€â”€ BenchmarkSuite.swift
â”‚   â”œâ”€â”€ Configuration/
â”‚   â”‚   â”œâ”€â”€ ConfigurationFactory.swift
â”‚   â”‚   â””â”€â”€ FrameworkConfigurations.swift
â”‚   â”œâ”€â”€ Logging/
â”‚   â”‚   â”œâ”€â”€ LLMService+Logging.swift
â”‚   â”‚   â””â”€â”€ Logger.swift
â”‚   â”œâ”€â”€ ModelManagement/
â”‚   â”‚   â”œâ”€â”€ ModelCompatibilityMatrix.swift
â”‚   â”‚   â”œâ”€â”€ ModelConverter.swift
â”‚   â”‚   â”œâ”€â”€ ModelFormatDetector.swift
â”‚   â”‚   â””â”€â”€ ModelRepository.swift
â”‚   â”œâ”€â”€ Monitoring/
â”‚   â”‚   â””â”€â”€ RealtimePerformanceMonitor.swift
â”‚   â”œâ”€â”€ Profiling/
â”‚   â”‚   â””â”€â”€ MemoryProfiler.swift
â”‚   â”œâ”€â”€ Testing/
â”‚   â”‚   â””â”€â”€ ABTestingFramework.swift
â”‚   â”œâ”€â”€ Tokenization/
â”‚   â”‚   â””â”€â”€ Tokenizer.swift
â”‚   â”œâ”€â”€ BenchmarkService.swift
â”‚   â”œâ”€â”€ BundledModelsService.swift
â”‚   â”œâ”€â”€ ConversationExporter.swift
â”‚   â”œâ”€â”€ ConversationStore.swift
â”‚   â”œâ”€â”€ DependencyContainer.swift
â”‚   â”œâ”€â”€ ExecuTorchService.swift
â”‚   â”œâ”€â”€ FrameworkConfiguration.swift
â”‚   â”œâ”€â”€ LLMError+Extended.swift
â”‚   â”œâ”€â”€ LLMError.swift
â”‚   â”œâ”€â”€ MLCService.swift
â”‚   â”œâ”€â”€ MemoryManager.swift
â”‚   â”œâ”€â”€ ModelCompatibilityChecker.swift
â”‚   â”œâ”€â”€ ModelLoader.swift
â”‚   â”œâ”€â”€ ModelManager.swift
â”‚   â”œâ”€â”€ ONNXService.swift
â”‚   â”œâ”€â”€ PerformanceMonitor.swift
â”‚   â”œâ”€â”€ PicoLLMService.swift
â”‚   â”œâ”€â”€ ServiceLifecycleObserverImpl.swift
â”‚   â”œâ”€â”€ SwiftTransformersService.swift
â”‚   â”œâ”€â”€ TFLiteService.swift
â”‚   â””â”€â”€ UnifiedLLMService.swift
â”œâ”€â”€ ViewModels/
â”‚   â”œâ”€â”€ ChatViewModel.swift
â”‚   â”œâ”€â”€ ChatViewModelEnhanced.swift
â”‚   â”œâ”€â”€ ComparisonViewModel.swift
â”‚   â”œâ”€â”€ FrameworkCapabilityExplorerViewModel.swift
â”‚   â”œâ”€â”€ ModelConversionWizardViewModel.swift
â”‚   â”œâ”€â”€ ModelListViewModel.swift
â”‚   â””â”€â”€ ModelQuantizationViewModel.swift
â”œâ”€â”€ Views/
â”‚   â”œâ”€â”€ Chat/
â”‚   â”‚   â””â”€â”€ ChatInterfaceView.swift
â”‚   â”œâ”€â”€ Comparison/
â”‚   â”‚   â””â”€â”€ ComparisonView.swift
â”‚   â”œâ”€â”€ Dashboard/
â”‚   â”‚   â”œâ”€â”€ Charts/
â”‚   â”‚   â”‚   â””â”€â”€ DashboardCharts.swift
â”‚   â”‚   â””â”€â”€ PerformanceDashboardView.swift
â”‚   â”œâ”€â”€ FrameworkExplorer/
â”‚   â”‚   â””â”€â”€ FrameworkCapabilityExplorerView.swift
â”‚   â”œâ”€â”€ ModelConversion/
â”‚   â”‚   â””â”€â”€ ModelConversionWizardView.swift
â”‚   â”œâ”€â”€ Quantization/
â”‚   â”‚   â””â”€â”€ ModelQuantizationView.swift
â”‚   â”œâ”€â”€ BenchmarkView.swift
â”‚   â”œâ”€â”€ ChatView.swift
â”‚   â”œâ”€â”€ MemoryMonitorView.swift
â”‚   â”œâ”€â”€ ModelDownloadView.swift
â”‚   â”œâ”€â”€ ModelImportView.swift
â”‚   â”œâ”€â”€ ModelListView.swift
â”‚   â”œâ”€â”€ ModelLoadingView.swift
â”‚   â””â”€â”€ SettingsView.swift
â”œâ”€â”€ Utilities/
â”‚   â””â”€â”€ Constants.swift
â””â”€â”€ ContentView.swift
```

## App Navigation

The app features a tab-based interface with four main sections:

1. **Chat Tab**: Interactive chat interface with streaming responses
2. **Models Tab**: Model management, loading, and framework selection  
3. **Benchmark Tab**: Performance testing and analytics tools
4. **Settings Tab**: Configuration options and advanced settings

## Getting Started

### Prerequisites
- **iOS 15.0+** (iOS 17.0+ recommended for full feature support)
- **Xcode 15.0+** 
- **Swift 5.9+**
- **macOS 12.0+** for development
- **Apple Silicon Mac** recommended for MLX framework testing

### Quick Start
1. Clone the repository and navigate to the iOS example:
   ```bash
   cd examples/ios/RunAnywhereAI/
   ```

2. Open the project in Xcode:
   ```bash
   open RunAnywhereAI.xcodeproj
   ```

3. Build and run the app on simulator or device

4. Explore the app:
   - **Chat Tab**: Start a conversation with the mock LLM service
   - **Models Tab**: Browse available frameworks and models
   - **Benchmark Tab**: Run performance tests and view analytics
   - **Settings Tab**: Configure generation parameters

## Development Setup

### Dependencies Installation
```bash
# Install CocoaPods dependencies (required for TensorFlow Lite)
pod install

# After pod install, always open the .xcworkspace file
open RunAnywhereAI.xcworkspace
```

### Important: Xcode 16 Sandbox Issues
When building with Xcode 16, you may encounter sandbox errors during the "Copy Pods Resources" build phase:
```
error: Sandbox: rsync(xxxxx) deny(1) file-write-create
```

**Solution**: After running `pod install`, run the fix script:
```bash
./fix_pods_sandbox.sh
```

This script automatically replaces the problematic `rsync` commands with `cp` to work around Xcode 16's stricter sandbox restrictions.

**Alternative manual fix**:
1. Open `Pods/Target Support Files/Pods-RunAnywhereAI/Pods-RunAnywhereAI-resources.sh`
2. Replace the `rsync` commands (around line 114) with `cp`:
   ```bash
   # Replace rsync with cp for sandbox compatibility
   while IFS= read -r file; do
     if [[ -n "$file" ]] && [[ -e "$file" ]]; then
       cp -R "$file" "${TARGET_BUILD_DIR}/${UNLOCALIZED_RESOURCES_FOLDER_PATH}/" || true
     fi
   done < "$RESOURCES_TO_COPY"
   ```

**Note**: This script is auto-generated by CocoaPods and will be overwritten on each `pod install`, so you'll need to apply this fix again after updating pods.

### Running Lints
```bash
# From the iOS example directory
./swiftlint.sh
```

### Building from Command Line
```bash
# Build for iOS Simulator
xcodebuild build -scheme RunAnywhereAI -destination 'platform=iOS Simulator,name=iPhone 15'

# Build for device (requires valid provisioning profile)
xcodebuild build -scheme RunAnywhereAI -destination 'generic/platform=iOS'
```

### Testing
```bash
# Run unit tests
xcodebuild test -scheme RunAnywhereAI -destination 'platform=iOS Simulator,name=iPhone 15'
```

## Framework Integration

This app demonstrates integration patterns for multiple LLM frameworks:

### Currently Implemented (Mock Services)
- **MockLLMService**: Development and testing service
- **Foundation Models**: System-level model APIs
- **Service Architecture**: Unified interface for all frameworks

### Integration-Ready Services
Each service includes the interface and architecture for:
- **llama.cpp**: GGUF model support with C++ bridge
- **Core ML**: Apple's native ML framework integration  
- **MLX**: Apple Silicon-optimized framework
- **ONNX Runtime**: Cross-platform model execution
- **TensorFlow Lite**: Mobile-optimized TensorFlow
- **ExecuTorch**: PyTorch mobile runtime
- **MLC LLM**: Machine learning compilation framework
- **Swift Transformers**: Pure Swift transformer implementation
- **PicoLLM**: Lightweight inference engine

### Adding Real Framework Support
To integrate actual frameworks:

1. **Add Dependencies**: Include the framework's Swift Package or library
2. **Update Service**: Replace mock implementation with real calls
3. **Configure Models**: Add model loading and format detection
4. **Test Integration**: Use the benchmark suite to validate performance

## Architecture Highlights

- **Modular Design**: Each framework is isolated in its own service
- **Dependency Injection**: Services are managed through DependencyContainer
- **Performance Monitoring**: Real-time metrics collection and analysis
- **Memory Management**: Intelligent resource allocation and cleanup
- **Error Handling**: Comprehensive error types and recovery strategies
- **Logging**: Structured logging for debugging and performance analysis

## License

This sample code is part of the RunAnywhereAI SDK project.
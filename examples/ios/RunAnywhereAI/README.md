# RunAnywhereAI iOS Sample App

A comprehensive iOS example app showcasing advanced on-device AI capabilities with multiple LLM frameworks, benchmarking tools, and performance optimization features.

ğŸ“– **For detailed features documentation, see [FEATURES.md](./FEATURES.md)**

## Features

### ğŸ¤– Multi-Framework Support
Integrates 10+ LLM frameworks including:
- **llama.cpp** (GGUF models)
- **Core ML** (Apple's native ML framework)
- **MLX** (Apple Silicon optimized)
- **ONNX Runtime** (Cross-platform inference)
- **TensorFlow Lite** (Mobile-optimized ML)
- **ExecuTorch** (PyTorch mobile runtime)
- **MLC LLM** (Machine Learning Compilation)
- **Swift Transformers** (Native Swift implementation)
- **PicoLLM** (Lightweight inference engine)
- **Foundation Models** (System-level APIs)

### ğŸ’¬ Advanced Chat Interface
- **Enhanced Chat View**: Full-featured chat UI with streaming responses
- **Chat Interface View**: Specialized chat components
- **Message Management**: Support for enhanced chat messages with metadata
- **Conversation Export**: Export chat histories
- **Conversation Store**: Persistent chat storage

### ğŸ“Š Performance & Benchmarking
- **Comprehensive Benchmarking Suite**: Multi-framework performance testing
- **Real-time Performance Monitoring**: Live metrics during inference
- **Memory Profiling**: Track memory usage and optimization
- **Performance Dashboard**: Visual charts and analytics
- **A/B Testing Framework**: Compare different configurations
- **Benchmark Service**: Automated performance testing

### ğŸ”§ Model Management & Conversion
- **Model Repository**: Centralized model storage and management
- **Model Compatibility Checker**: Validate model-framework compatibility
- **Model Conversion Wizard**: Convert between different model formats
- **Model Format Detection**: Automatic format identification
- **Model Quantization**: Optimize models for mobile deployment
- **Model Import/Download**: Easy model acquisition and management
- **Bundled Models Service**: Pre-packaged model management

### ğŸš€ Framework Exploration
- **Framework Capability Explorer**: Interactive framework comparison
- **Model Compatibility Matrix**: Visual compatibility overview
- **Framework Configurations**: Optimized settings per framework
- **Dependency Container**: Modular service architecture

### âš™ï¸ Advanced Configuration
- **Settings View**: Comprehensive configuration options
- **Generation Options**: Fine-tune inference parameters
- **Memory Management**: Intelligent memory allocation
- **Service Lifecycle Management**: Proper resource handling
- **Logging System**: Comprehensive debug and performance logging

## Project Structure

```
RunAnywhereAI/
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ ChatMessage.swift
â”‚   â”œâ”€â”€ ChatMessageEnhanced.swift
â”‚   â”œâ”€â”€ GenerationOptions.swift
â”‚   â””â”€â”€ ModelInfo.swift
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ LLMService/
â”‚   â”‚   â”œâ”€â”€ BaseLLMService.swift
â”‚   â”‚   â”œâ”€â”€ CoreMLService.swift
â”‚   â”‚   â”œâ”€â”€ FoundationModelsService.swift
â”‚   â”‚   â”œâ”€â”€ FrameworkInfo.swift
â”‚   â”‚   â”œâ”€â”€ LLMCapabilities.swift
â”‚   â”‚   â”œâ”€â”€ LLMInference.swift
â”‚   â”‚   â”œâ”€â”€ LLMMetrics.swift
â”‚   â”‚   â”œâ”€â”€ LLMModelLoader.swift
â”‚   â”‚   â”œâ”€â”€ LLMProtocol.swift
â”‚   â”‚   â”œâ”€â”€ LlamaCppService.swift
â”‚   â”‚   â””â”€â”€ MLXService.swift
â”‚   â”œâ”€â”€ Benchmarking/
â”‚   â”‚   â””â”€â”€ BenchmarkSuite.swift
â”‚   â”œâ”€â”€ Configuration/
â”‚   â”‚   â”œâ”€â”€ ConfigurationFactory.swift
â”‚   â”‚   â””â”€â”€ FrameworkConfigurations.swift
â”‚   â”œâ”€â”€ Logging/
â”‚   â”‚   â”œâ”€â”€ LLMService+Logging.swift
â”‚   â”‚   â””â”€â”€ Logger.swift
â”‚   â”œâ”€â”€ ModelManagement/
â”‚   â”‚   â”œâ”€â”€ ModelCompatibilityMatrix.swift
â”‚   â”‚   â”œâ”€â”€ ModelConverter.swift
â”‚   â”‚   â”œâ”€â”€ ModelFormatDetector.swift
â”‚   â”‚   â””â”€â”€ ModelRepository.swift
â”‚   â”œâ”€â”€ Monitoring/
â”‚   â”‚   â””â”€â”€ RealtimePerformanceMonitor.swift
â”‚   â”œâ”€â”€ Profiling/
â”‚   â”‚   â””â”€â”€ MemoryProfiler.swift
â”‚   â”œâ”€â”€ Testing/
â”‚   â”‚   â””â”€â”€ ABTestingFramework.swift
â”‚   â”œâ”€â”€ Tokenization/
â”‚   â”‚   â””â”€â”€ Tokenizer.swift
â”‚   â”œâ”€â”€ BenchmarkService.swift
â”‚   â”œâ”€â”€ BundledModelsService.swift
â”‚   â”œâ”€â”€ ConversationExporter.swift
â”‚   â”œâ”€â”€ ConversationStore.swift
â”‚   â”œâ”€â”€ DependencyContainer.swift
â”‚   â”œâ”€â”€ ExecuTorchService.swift
â”‚   â”œâ”€â”€ FrameworkConfiguration.swift
â”‚   â”œâ”€â”€ LLMError+Extended.swift
â”‚   â”œâ”€â”€ LLMError.swift
â”‚   â”œâ”€â”€ MLCService.swift
â”‚   â”œâ”€â”€ MemoryManager.swift
â”‚   â”œâ”€â”€ ModelCompatibilityChecker.swift
â”‚   â”œâ”€â”€ ModelLoader.swift
â”‚   â”œâ”€â”€ ModelManager.swift
â”‚   â”œâ”€â”€ ONNXService.swift
â”‚   â”œâ”€â”€ PerformanceMonitor.swift
â”‚   â”œâ”€â”€ PicoLLMService.swift
â”‚   â”œâ”€â”€ ServiceLifecycleObserverImpl.swift
â”‚   â”œâ”€â”€ SwiftTransformersService.swift
â”‚   â”œâ”€â”€ TFLiteService.swift
â”‚   â””â”€â”€ UnifiedLLMService.swift
â”œâ”€â”€ ViewModels/
â”‚   â”œâ”€â”€ ChatViewModel.swift
â”‚   â”œâ”€â”€ ChatViewModelEnhanced.swift
â”‚   â”œâ”€â”€ ComparisonViewModel.swift
â”‚   â”œâ”€â”€ FrameworkCapabilityExplorerViewModel.swift
â”‚   â”œâ”€â”€ ModelConversionWizardViewModel.swift
â”‚   â”œâ”€â”€ ModelListViewModel.swift
â”‚   â””â”€â”€ ModelQuantizationViewModel.swift
â”œâ”€â”€ Views/
â”‚   â”œâ”€â”€ Chat/
â”‚   â”‚   â””â”€â”€ ChatInterfaceView.swift
â”‚   â”œâ”€â”€ Comparison/
â”‚   â”‚   â””â”€â”€ ComparisonView.swift
â”‚   â”œâ”€â”€ Dashboard/
â”‚   â”‚   â”œâ”€â”€ Charts/
â”‚   â”‚   â”‚   â””â”€â”€ DashboardCharts.swift
â”‚   â”‚   â””â”€â”€ PerformanceDashboardView.swift
â”‚   â”œâ”€â”€ FrameworkExplorer/
â”‚   â”‚   â””â”€â”€ FrameworkCapabilityExplorerView.swift
â”‚   â”œâ”€â”€ ModelConversion/
â”‚   â”‚   â””â”€â”€ ModelConversionWizardView.swift
â”‚   â”œâ”€â”€ Quantization/
â”‚   â”‚   â””â”€â”€ ModelQuantizationView.swift
â”‚   â”œâ”€â”€ BenchmarkView.swift
â”‚   â”œâ”€â”€ ChatView.swift
â”‚   â”œâ”€â”€ MemoryMonitorView.swift
â”‚   â”œâ”€â”€ ModelDownloadView.swift
â”‚   â”œâ”€â”€ ModelImportView.swift
â”‚   â”œâ”€â”€ ModelListView.swift
â”‚   â”œâ”€â”€ ModelLoadingView.swift
â”‚   â””â”€â”€ SettingsView.swift
â”œâ”€â”€ Utilities/
â”‚   â””â”€â”€ Constants.swift
â””â”€â”€ ContentView.swift
```

## App Navigation

The app features a tab-based interface with four main sections:

1. **Chat Tab**: Interactive chat interface with streaming responses
2. **Models Tab**: Model management, loading, and framework selection
3. **Benchmark Tab**: Performance testing and analytics tools
4. **Settings Tab**: Configuration options and advanced settings

## Getting Started

### Prerequisites
- **iOS 15.0+** (iOS 17.0+ recommended for full feature support)
- **Xcode 15.0+**
- **Swift 5.9+**
- **macOS 12.0+** for development
- **Apple Silicon Mac** recommended for MLX framework testing

### Quick Start

#### Option 1: Using Build Scripts (Recommended)
1. Clone the repository and navigate to the iOS example:
   ```bash
   cd examples/ios/RunAnywhereAI/
   ```

2. Build and run on simulator:
   ```bash
   ./scripts/build_and_run.sh simulator "iPhone 16 Pro"
   ```

3. Or build and run on connected device:
   ```bash
   ./scripts/build_and_run.sh device
   ```

#### Option 2: Using Xcode
1. Install dependencies and open workspace:
   ```bash
   pod install
   ./fix_pods_sandbox.sh  # Required for Xcode 16
   open RunAnywhereAI.xcworkspace
   ```

2. Build and run the app on simulator or device

#### Explore the App
- **Chat Tab**: Start a conversation with an LLM service
- **Models Tab**: Browse available frameworks and download models
- **Benchmark Tab**: Run performance tests and view analytics
- **Settings Tab**: Configure generation parameters

## Development Setup

### Dependencies Installation
```bash
# Install CocoaPods dependencies (required for TensorFlow Lite)
pod install

# After pod install, always open the .xcworkspace file
open RunAnywhereAI.xcworkspace
```

### Important: Xcode 16 Sandbox Issues
When building with Xcode 16, you may encounter sandbox errors during the "Copy Pods Resources" build phase:
```
error: Sandbox: rsync(xxxxx) deny(1) file-write-create
```

**Solution**: After running `pod install`, run the fix script:
```bash
./fix_pods_sandbox.sh
```

This script automatically replaces the problematic `rsync` commands with `cp` to work around Xcode 16's stricter sandbox restrictions.

**Alternative manual fix**:
1. Open `Pods/Target Support Files/Pods-RunAnywhereAI/Pods-RunAnywhereAI-resources.sh`
2. Replace the `rsync` commands (around line 114) with `cp`:
   ```bash
   # Replace rsync with cp for sandbox compatibility
   while IFS= read -r file; do
     if [[ -n "$file" ]] && [[ -e "$file" ]]; then
       cp -R "$file" "${TARGET_BUILD_DIR}/${UNLOCALIZED_RESOURCES_FOLDER_PATH}/" || true
     fi
   done < "$RESOURCES_TO_COPY"
   ```

**Note**: This script is auto-generated by CocoaPods and will be overwritten on each `pod install`, so you'll need to apply this fix again after updating pods.

### Swift Macro Support Setup

This project integrates **llm.swift** which uses Swift Macros for LLM model definitions and prompt handling. If you encounter macro fingerprint validation errors or macro-related build issues, follow these steps:

#### 1. Enable Macro Fingerprint Validation Skip
Run this command to disable macro fingerprint validation in Xcode:
```bash
defaults write com.apple.dt.Xcode IDESkipMacroFingerprintValidation -bool YES
```

#### 2. Configure Build Settings
In your Xcode project:
1. Select your target in the Project Navigator
2. Go to **Build Settings**
3. Search for **"Other Swift Flags"**
4. Add the following flag: `-enable-experimental-feature Macros`

#### Why This Is Needed
llm.swift uses Swift Macros for code generation, particularly for:
- LLM model definitions and configurations
- Prompt template processing and generation
- Type-safe model parameter handling

Without these settings, you may encounter build errors related to macro expansion or fingerprint validation.

### Running Lints
```bash
# From the iOS example directory
./swiftlint.sh
```

```swiftlint --fix --format```

### Building from Command Line

#### Using Build Scripts (Recommended)
```bash
# Build and run on simulator
./scripts/build_and_run.sh simulator "iPhone 16 Pro"

# Build and run on connected device
./scripts/build_and_run.sh device "Your Device Name"

# Clean build artifacts
./scripts/clean_build_and_run.sh
```

#### Manual Xcode Commands
```bash
# Build for iOS Simulator
xcodebuild -workspace RunAnywhereAI.xcworkspace -scheme RunAnywhereAI -destination 'platform=iOS Simulator,name=iPhone 16 Pro' build

# Build for device (requires valid provisioning profile)
xcodebuild -workspace RunAnywhereAI.xcworkspace -scheme RunAnywhereAI -destination 'generic/platform=iOS' build
```

### Testing
```bash
# Run unit tests
xcodebuild test -workspace RunAnywhereAI.xcworkspace -scheme RunAnywhereAI -destination 'platform=iOS Simulator,name=iPhone 16 Pro'
```

### Model URL Verification
The app includes a comprehensive model download system with URL verification:

```bash
# Verify all model download URLs
./scripts/verify_urls.sh
```

This script:
- âœ… Uses `ModelURLRegistry.swift` as the single source of truth
- ğŸ” Verifies accessibility of all download URLs
- ğŸ“Š Provides detailed success/failure reporting
- âš ï¸ Handles authentication-required URLs (Kaggle models)
- ğŸ”„ Must be run from the `scripts/` directory

## Framework Integration

This app demonstrates integration patterns for multiple LLM frameworks:

### Currently Available Services
- **Core ML**: Apple's native ML framework (iOS 17.0+)
- **MLX**: Apple Silicon-optimized framework (iOS 17.0+)
- **ONNX Runtime**: Cross-platform inference engine
- **TensorFlow Lite**: Mobile-optimized TensorFlow
- **Foundation Models**: System-level model APIs (iOS 18.0+)

### Integration-Ready Services
Each service includes the interface and architecture for:
- **llama.cpp**: GGUF model support with C++ bridge
- **Core ML**: Apple's native ML framework integration
- **MLX**: Apple Silicon-optimized framework
- **ONNX Runtime**: Cross-platform model execution
- **TensorFlow Lite**: Mobile-optimized TensorFlow
- **ExecuTorch**: PyTorch mobile runtime
- **MLC LLM**: Machine learning compilation framework
- **Swift Transformers**: Pure Swift transformer implementation
- **PicoLLM**: Lightweight inference engine

### Adding Real Framework Support
To integrate actual frameworks:

1. **Add Dependencies**: Include the framework's Swift Package or library
2. **Update Service**: Replace mock implementation with real calls
3. **Configure Models**: Add model loading and format detection
4. **Test Integration**: Use the benchmark suite to validate performance

## Documentation

### Guides
- **[Bundled Models Guide](RunAnywhereAI/docs/BUNDLED_MODELS_GUIDE.md)**: How to add pre-trained models to your app bundle
- **[Model Download Guide](docs/MODEL_DOWNLOAD_GUIDE.md)**: On-demand model download system
- **[Model Integration Guide](docs/MODEL_INTEGRATION_GUIDE.md)**: Complete model integration documentation
- **[Foundation Models Setup](docs/FOUNDATION_MODELS_SETUP.md)**: iOS 18+ foundation models
- **[TensorFlow Lite Fix](docs/TENSORFLOW_LITE_FIX.md)**: Resolving TFLite issues

## Architecture Highlights

- **Modular Design**: Each framework is isolated in its own service
- **Dependency Injection**: Services are managed through DependencyContainer
- **Performance Monitoring**: Real-time metrics collection and analysis
- **Memory Management**: Intelligent resource allocation and cleanup
- **Error Handling**: Comprehensive error types and recovery strategies
- **Logging**: Structured logging for debugging and performance analysis

## License

This sample code is part of the RunAnywhereAI SDK project.
